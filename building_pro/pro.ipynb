{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a11ffcd7",
   "metadata": {},
   "source": [
    "> This notebook is an extension of Micrograd. We’ll add some cool features and enhancements to the core engine. It assumes you already understand the core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020dd38f",
   "metadata": {},
   "source": [
    "# <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Config</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa119977",
   "metadata": {},
   "source": [
    "make sure you installed requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "42697c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "sys.path.append('../core')\n",
    "from engine import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "dbabc4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am also using custom seaborn style simply for better aesthetics\n",
    "# you can ignore this part\n",
    "# Gruvbox-inspired Seaborn configuration\n",
    "from cycler import cycler\n",
    "\n",
    "gruvbox_colors = [\n",
    "    \"#fb4934\",  # red\n",
    "    \"#fabd2f\",  # yellow\n",
    "    \"#b8bb26\",  # green\n",
    "    \"#83a598\",  # blue\n",
    "    \"#d3869b\",  # purple\n",
    "    \"#fe8019\",  # orange\n",
    "    \"#8ec07c\",  # aqua\n",
    "]\n",
    "\n",
    "rc = {\n",
    "    \"axes.facecolor\": \"#282828\",   # Dark background\n",
    "    \"figure.facecolor\": \"#282828\", # Match figure bg\n",
    "    \"axes.edgecolor\": \"#3c3836\",   # Softer border\n",
    "    \"grid.color\": \"#504945\",       # Subtle grid lines\n",
    "    \"font.family\": \"arial\",\n",
    "    \"axes.labelcolor\": \"#ebdbb2\",  # Warm text\n",
    "    \"xtick.color\": \"#d5c4a1\",\n",
    "    \"ytick.color\": \"#d5c4a1\",\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"axes.prop_cycle\": cycler(color=gruvbox_colors), \n",
    "}\n",
    "\n",
    "sns.set(rc=rc, style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5002514f",
   "metadata": {},
   "source": [
    "# <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Activation Functions</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c22633",
   "metadata": {},
   "source": [
    "In Andrew’s videos and repo, we already have tanh and ReLU implemented. These are simple, widely-used activations, but for deeper networks or transformers, more advanced activations can improve learning and gradient flow.\n",
    "\n",
    "I will add:\n",
    "\n",
    "* Leaky ReLU - fixes the \"dying ReLU\" problem\n",
    "* GELU - smooth, probabilistic activation used in Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd2c25",
   "metadata": {},
   "source": [
    "## What is Leaky ReLU?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778903a6",
   "metadata": {},
   "source": [
    "\n",
    "Lealy ReLU is a small modification of ReLU that allows a tiny, non-zero gradient when the input is negative:\n",
    "\n",
    "\n",
    "\n",
    "f(x) =\n",
    "\\begin{cases} \n",
    "x & \\text{if } x \\ge 0 \\\\\n",
    "\\alpha x & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "\n",
    "\n",
    "so the **derivative** is:\n",
    "\n",
    "f'(x) =\n",
    "\\begin{cases} \n",
    "1 & \\text{if } x \\ge 0 \\\\\n",
    "\\alpha & \\text{if } x < 0\n",
    "\\end{cases}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60930943",
   "metadata": {},
   "source": [
    "**Note:** Standard ReLU can \"die\" if a neuron only outputs negative values. Basically its gradient becomes 0, and learning stops. To avoid this Leaky ReLU helps us. It keeps small gradient (alpha) for negative values, thus improving training stability.\n",
    "\n",
    "**Reference:** https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29623e33",
   "metadata": {},
   "source": [
    "![Leaky ReLU](../pics/LReLU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f89cefd",
   "metadata": {},
   "source": [
    "How LReLU looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "18086b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1280x960 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pretty easy to visualize with matplotlib\n",
    "# assume alpha is 0.01 (it can be any small value)\n",
    "xs = np.linspace(-10, 10, 100)\n",
    "ys = np.where(xs > 0, xs, 0.01 * xs)\n",
    "plt.figure(figsize=(16, 12), dpi=80)\n",
    "plt.plot(xs, ys)\n",
    "plt.title(\"Leaky ReLU Activation Function\", fontsize=16)\n",
    "# set 0 a bit higher to see the negative slope\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlim(-10, 10)\n",
    "plt.axhline(0, color='white', lw=0.5)\n",
    "plt.axvline(0, color='white', lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a13986f",
   "metadata": {},
   "source": [
    "Just to see the difference, standard ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "363b1574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1280x960 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For comparece, here is standard ReLU\n",
    "ys_relu = np.where(xs > 0, xs, 0)\n",
    "plt.figure(figsize=(16, 12), dpi=80)\n",
    "plt.plot(xs, ys_relu)\n",
    "plt.title(\"Leaky ReLU Activation Function\", fontsize=16, color='white')\n",
    "# set 0 a bit higher to see the negative slope\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlim(-10, 10)\n",
    "plt.axhline(0, color='white', lw=0.5)\n",
    "plt.axvline(0, color='white', lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4269985",
   "metadata": {},
   "source": [
    "### Implementation of Leaky ReLU for micrograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "15cb3276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=-0.04, grad=1)\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "#in micrograd, we can implement it like this, as a new method in the Value class:\n",
    "\n",
    "# since formula is f(x) = x if x > 0 else alpha * x\n",
    "# derivative is f'(x) = 1 if x > 0 else alpha\n",
    "\n",
    "def leaky_relu(self, alpha=0.01):\n",
    "    out = Value(self.data if self.data > 0 else alpha * self.data, (self,), 'lrelu')\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += (1 if self.data > 0 else alpha) * out.grad # don't forget chain rule\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "\n",
    "# we can quickly test it (since it is implemented in my Value class, I can use it directly): \n",
    "a = Value(-4.0)\n",
    "b = a.leaky_relu()\n",
    "b.backward()\n",
    "print(b)  # should be -0.04\n",
    "print(a.grad)  # should be 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbaa1a2",
   "metadata": {},
   "source": [
    "## What is GELU?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb682411",
   "metadata": {},
   "source": [
    "GELU is Gaussian Error Linear Unit - sounds scary. I will try to explain it. Intuitionally you can think of it like this:\n",
    "\n",
    "    “the bigger x is, the more it should contribute, but in a smooth, probabilistic way.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c516e",
   "metadata": {},
   "source": [
    "### Mathematical Foundation of GELU:\n",
    "\n",
    "The GELU (Gaussian Error Linear Unit) activation function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = x \\cdot \\Phi(x)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "\n",
    "- $x$ is the input,  \n",
    "- $\\Phi(x)$ is the cumulative distribution function (CDF) of the standard normal distribution. By definition CDF is just a distribution function $F_X(x)$ of $X$, evaluated at $x$ that shows the probability that $X$ will take a value **less than or equal to** $x$ \n",
    "\n",
    "$$ F_X(x) = P(X \\le x) $$\n",
    "\n",
    "  \n",
    "- For reference on the standard normal CDF: [probability course](https://www.probabilitycourse.com/chapter4/4_2_3_normal.php)\n",
    "\n",
    "- and [wiki page](https://en.wikipedia.org/wiki/Normal_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be94e07",
   "metadata": {},
   "source": [
    "### Expanding Idea Further\n",
    "\n",
    "Now the idea is that this CDF can be approximated using the error function\n",
    "\n",
    "$$\n",
    "\\Phi(x) = \\frac{1}{2} \\left[ 1 + \\text{erf}\\left( \\frac{x}{\\sqrt{2}} \\right) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "So, GELU can be written as:\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = 0.5 \\cdot x \\left[ 1 + \\text{erf}\\left( \\frac{x}{\\sqrt{2}} \\right) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "**Why approximation??** Because then this approximation is **deterministic** because:\n",
    "\n",
    "- The error function is a well-defined mathematical function, not a distribution function.  \n",
    "- For any given input \\(x\\), the output is fixed and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b4cc1",
   "metadata": {},
   "source": [
    "### Deterministic Nature of GELU\n",
    "\n",
    "Nice, now we understand why despite its probabilistic-inspired formulation GELU is **deterministic**\n",
    "\n",
    "\n",
    "**Note:** Being deterministic is **essential** because it allows us to:\n",
    "\n",
    "- Compute derivatives reliably  \n",
    "- Ensure consistent behavior during training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca83c39",
   "metadata": {},
   "source": [
    "**Reference:** you can find the paper on GELU here https://arxiv.org/pdf/1606.08415v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf5678",
   "metadata": {},
   "source": [
    "From pictures taken from this paper you can see visualization of GELU and how it outperforms other activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bca24",
   "metadata": {},
   "source": [
    "![GELU](../pics/gelu_from_paper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0664e0ff",
   "metadata": {},
   "source": [
    "![GELU_MNIST](../pics/gelu_mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4daed9c",
   "metadata": {},
   "source": [
    "Let's also plot GELU here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0d2c7118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1280x960 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's also plot GELU here: \n",
    "def gelu(x):\n",
    "    \"\"\"Gaussian Error Linear Unit activation function.\"\"\"\n",
    "    # This is how aproximation looks like from paper. You can read more in the reference section\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * (x ** 3))))\n",
    "\n",
    "ys_gelu = gelu(xs)\n",
    "plt.figure(figsize=(16, 12), dpi=80)\n",
    "plt.plot(xs, ys_gelu)\n",
    "plt.title(\"GELU Activation Function\", fontsize=16, color='white')\n",
    "# set 0 a bit higher to see the negative slope\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlim(-10, 10)\n",
    "plt.axhline(0, color='white', lw=0.5)\n",
    "plt.axvline(0, color='white', lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34911b",
   "metadata": {},
   "source": [
    "### Implementation of GELU for micrograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a2a228a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can implement it in micrograd as well:\n",
    "# this would be a bit tricky since we need to implement tanh as well (Andrew has it in vidoes but not on gitHub) \n",
    "def tanh(self):\n",
    "    x = self.data\n",
    "    t = (np.exp(2 * x) - 1) / (np.exp(2 * x) + 1)\n",
    "    out = Value(t, (self,), 'tanh')\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += (1 - t ** 2) * out.grad  # derivative of tanh is 1 - tanh^2(x)\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "\n",
    "# now we can implement GELU using this tanh\n",
    "def gelu(self):\n",
    "    \"\"\"Gaussian Error Linear Unit activation function.\"\"\"\n",
    "    # This is how aproximation looks like from paper. You can read more in the reference section\n",
    "    x = self.data\n",
    "    tanh_out = (np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * (x ** 3)))) # calculate tanh part\n",
    "    out = Value(0.5 * x * (1 + tanh_out), (self,), 'gelu')\n",
    "\n",
    "    def _backward():\n",
    "        # derivative of GELU is a quite a bit complex, so remember calculus classes!\n",
    "        # since we have 0.5x * tanh_out that also uses x, so we use the chain rule here\n",
    "        # don't forget that tanh_out is a complex function of x as well\n",
    "        # derivative is gonna be:\n",
    "        # 0.5 * (1 + tanh_out) + 0.5 * x * dtanh/dx \n",
    "        # where dtanh/dx = (1 - tanh^2(x)) * d(inner)/dx\n",
    "        # and d(inner)/dx = sqrt(2/pi) * (1 + 3 * 0.044715 * x^2)\n",
    "        dtanh = (1 - tanh_out ** 2)  # derivative of tanh\n",
    "        dgelu_dx = 0.5 * (1 + tanh_out) + 0.5 * x * dtanh * np.sqrt(2 / np.pi) * (1 + 3 * 0.044715 * (x ** 2))\n",
    "        self.grad += dgelu_dx * out.grad\n",
    "        \n",
    "    out._backward = _backward\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1861198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.8411919906082768, grad=1)\n",
      "1.0829640838457826\n"
     ]
    }
   ],
   "source": [
    "# it's a bit to calculate manually, but we can test it like this:\n",
    "a = Value(1.0)\n",
    "b = a.gelu()\n",
    "b.backward()\n",
    "print(b)  # should be around 0.841\n",
    "print(a.grad)  # should be around 1.083\n",
    "\n",
    "# Actual calculations:\n",
    "\n",
    "# For tanh_out x = 1.0:\n",
    "# tanh_out = tanh(sqrt(2/pi) * (1 + 0.044715 * 1^3)) = tanh(0.7978845608 * 1.044715) = tanh(0.833)\n",
    "# tanh(0.833) = 0.681\n",
    "# So, b = 0.5 * 1 * (1 + 0.681) = 0.5 * 1.681 = 0.8405 (approximately 0.841)\n",
    "\n",
    "# For gradient:\n",
    "# dtanh = 1 - tanh_out^2 = 1 - 0.681^2 = 1 - 0.464761 = 0.535239\n",
    "# d(inner)/dx = sqrt(2/pi) * (1 + 3 * 0.044715 * 1^2) = 0.7978845608 * (1 + 0.134145) = 0.7978845608 * 1.134145 = 0.904\n",
    "# dgelu_dx = 0.5 * (1 + 0.681) + 0.5 * 1 * 0.535239 * 0.904 = 0.5 * 1.681 + 0.5 * 0.484 = 0.8405 + 0.242 = 1.082\n",
    "# So, a.grad = 1.082 (approximately 1.083)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f652bfad",
   "metadata": {},
   "source": [
    "# <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Enhanced MLP Components</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e76b597",
   "metadata": {},
   "source": [
    "Next I would like to enhance MLP components or `nn.py` file of micrograd. Specifically add those:\n",
    "\n",
    "- **Base Layer:** all layers inherit a parameters() method\n",
    "- **Linear layer:** funny connected layer\n",
    "- **Sequential container:** chain layers together\n",
    "- **LayerNorm:** stabilize training by normalizing activations\n",
    "- **Dropout:** regularization\n",
    "\n",
    "We will get throught all of them one by one!\n",
    "By the end, even thought these additions make the architecture slightly more complex, they allow faster convergence, smoother gradients, and better generalization!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374459be",
   "metadata": {},
   "source": [
    "### Base Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a20c6",
   "metadata": {},
   "source": [
    "Base Layer is the simplest and an abstract layer. All the others will inherit from it.\n",
    "\n",
    "* It provides a `parameters()` method for collecting all trainable parameters of a layer\n",
    "* So we can, for example, access `update(p) for p in model.parameters()`\n",
    "\n",
    "By having this base layer, any new layer we add, whether Linear, LayerNorm, Dropout or even something else, will follow the same micrograd principlese of tracking `Value` objects and building the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a2151109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To implement base layer class we can do something like this:\n",
    "class Layer:\n",
    "    def parameters(self):\n",
    "        return []  # By default, no parameters\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955adb0",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa10eb",
   "metadata": {},
   "source": [
    "Linear Layer (also known as fully connected or dense layer) would be the modern equivalent of Andrew's `Neuron`. Key difference, instead of handling one neuron at a time, a Linear layer computes a fully connected transformation from the input to the output.\n",
    "\n",
    "It stores all weights and biases as `Value` objects, preserving the gradient tracking. Additionally Nonlinearities are separated from the Linear layer, which allows us to plug in any activation function we like (ReLU, LeakyReLU, GELU, etc.), making the network more modular and easier to experiment with.\n",
    "\n",
    "**Note:**\n",
    "* PyTorch has Linear layer: https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "* There is also a great documentation by NVIDIA: https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be53d2",
   "metadata": {},
   "source": [
    "![nvidia_linear](../pics/nvidia_linear.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9645fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement Linear layer for our micrograd pro:\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        self.weights = [Value(np.random.uniform(-1, 1)) for _ in range(in_features * out_features)]\n",
    "        self.bias = [Value(0.0) for _ in range(out_features)] if bias else None\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.has_bias = bias # decided to add this just like in PyTorch since it's a useful option for some layers\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        out = []\n",
    "        for i in range(self.out_features):\n",
    "            # Compute the dot product for the i-th output neuron\n",
    "            start = i * self.in_features\n",
    "            end = start + self.in_features\n",
    "            neuron_weights = self.weights[start:end]\n",
    "            neuron_output = sum(w * xi for w, xi in zip(neuron_weights, x))\n",
    "            if self.has_bias:\n",
    "                neuron_output += self.bias[i]\n",
    "            out.append(neuron_output)\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        # Collect weights and biases\n",
    "        return super().parameters() + self.weights + (self.bias if self.has_bias else [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947cab1b",
   "metadata": {},
   "source": [
    "However we are missing one important component here!\n",
    "\n",
    "Right now `Linear` layer computes outputs using `Value` objectsm, so **forward pass** works, but the outputs **don't** define their own `_backward` function that distributes gradients back to `weights`, `bias`, and `x`.\n",
    "\n",
    "In other words, we are missing necessary `_backward` defined for each `Value`, since autograd only knows how to backproparage through operations if each output knows its **dependency** on the inputs.\n",
    "\n",
    "Here is how I decided to implement it, to track gradients for weights, biases and inputs, keeping micrograd philosophy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "68e1e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        self.weights = [Value(np.random.uniform(-1, 1)) for _ in range(in_features * out_features)]\n",
    "        self.bias = [Value(0.0) for _ in range(out_features)] if bias else None\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.has_bias = bias # decided to add this just like in PyTorch since it's a useful option for some layers\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        out = []\n",
    "        for i in range(self.out_features):\n",
    "            # Compute the dot product for the i-th output neuron\n",
    "            start = i * self.in_features\n",
    "            end = start + self.in_features\n",
    "            neuron_weights = self.weights[start:end]\n",
    "            neuron_output = sum(w * xi for w, xi in zip(neuron_weights, x))\n",
    "            if self.has_bias:\n",
    "                neuron_output += self.bias[i]\n",
    "            \n",
    "            # Wrap neuron_output in Value with _prev and _backward\n",
    "            def make_backward(neuron_weights=neuron_weights, neuron_output=neuron_output, x=x, bias=self.bias[i] if self.has_bias else None):\n",
    "                # capture local variables in default to bind in closure\n",
    "                out_val = neuron_output\n",
    "                def _backward():\n",
    "                    # propagete gradients to weights, inputs, and bias\n",
    "                    for w, xi in zip(neuron_weights, x):\n",
    "                        w.grad += xi.data * out_val.grad  # Gradient w.r.t. weights\n",
    "                        xi.grad += w.data * out_val.grad  # Gradient w.r.t. inputs\n",
    "                    if self.has_bias:\n",
    "                        bias.grad += out_val.grad  # Gradient w.r.t. bias\n",
    "                out_val._backward = _backward\n",
    "                return out_val\n",
    "            \n",
    "            out.append(make_backward())\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        # Collect weights and biases\n",
    "        return super().parameters() + self.weights + (self.bias if self.has_bias else [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c310c30",
   "metadata": {},
   "source": [
    "Thus:\n",
    "\n",
    "1. Each output neuron `Value` now knows which **weight**, **bias**, and **input** contributed to it (`_prev` is set implicitly via closure)\n",
    "2. `_backward` implements the **chain rule**, distributing the gradient from the output to each weight, bias, and input\n",
    "3. When you call `loss.backward()`, the gradients flow correctly thought all parameters, just like in micrograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a53761e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [Value(data=0.14753925386048428, grad=0), Value(data=-1.1374958763022087, grad=0)]\n",
      "Output grads before backward: [0, 0]\n",
      "Weight 0 gradient: 1.0\n",
      "Weight 1 gradient: -2.0\n",
      "Weight 2 gradient: 3.0\n",
      "Weight 3 gradient: 1.0\n",
      "Weight 4 gradient: -2.0\n",
      "Weight 5 gradient: 3.0\n",
      "Bias 0 gradient: 1\n",
      "Bias 1 gradient: 1\n",
      "Input 0 gradient: 0.4565201591641068\n",
      "Input 1 gradient: 0.9729644179667623\n",
      "Input 2 gradient: 0.16648401810923108\n"
     ]
    }
   ],
   "source": [
    "# LET'S TEST IT!\n",
    "# Create a Linear layer with 3 inputs and 2 outputs\n",
    "layer = Linear(3, 2)\n",
    "# Create a sample input (3 features)\n",
    "x = [Value(1.0), Value(-2.0), Value(3.0)]\n",
    "\n",
    "# --- First Pass ---\n",
    "\n",
    "# Forward pass\n",
    "out = layer(x)\n",
    "print(\"Output:\", out)\n",
    "print(\"Output grads before backward:\", [o.grad for o in out])\n",
    "# Assume some loss (sum of outputs for simplicity)\n",
    "loss = out[0] + out[1]\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "# Print gradients\n",
    "# weights, biases, and inputs should have gradients now\n",
    "for i, w in enumerate(layer.weights):\n",
    "    print(f\"Weight {i} gradient: {w.grad}\")\n",
    "if layer.has_bias:\n",
    "    for i, b in enumerate(layer.bias):\n",
    "        print(f\"Bias {i} gradient: {b.grad}\")\n",
    "for i, xi in enumerate(x):\n",
    "    print(f\"Input {i} gradient: {xi.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d3016f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (2nd pass): [Value(data=0.14753925386048428, grad=0), Value(data=-1.1374958763022087, grad=0)]\n",
      "Output grads before backward (2nd pass): [0, 0]\n",
      "Gradients after 2nd pass:\n",
      "Weight 0 gradient: 0.5\n",
      "Weight 1 gradient: -1.0\n",
      "Weight 2 gradient: 1.5\n",
      "Weight 3 gradient: 1.0\n",
      "Weight 4 gradient: -2.0\n",
      "Weight 5 gradient: 3.0\n",
      "Bias 0 gradient: 0.5\n",
      "Bias 1 gradient: 1\n",
      "Input 0 gradient: 0.3202302259383699\n",
      "Input 1 gradient: 0.5051906095204712\n",
      "Input 2 gradient: -0.12452508542313134\n"
     ]
    }
   ],
   "source": [
    "# --- Second Pass ---\n",
    "# Zero gradients\n",
    "layer.zero_grad()\n",
    "for xi in x:\n",
    "    xi.grad = 0\n",
    "# Forward pass again\n",
    "out = layer(x)\n",
    "print(\"Output (2nd pass):\", out)\n",
    "print(\"Output grads before backward (2nd pass):\", [o.grad for o in out])\n",
    "# Assume some loss change \n",
    "loss = out[0] + out[1] - 0.5 * out[0]  # just to change the loss a bit\n",
    "# Backward pass second pass\n",
    "loss.backward()\n",
    "\n",
    "# Print gradients after second pass\n",
    "print(\"Gradients after 2nd pass:\")\n",
    "for i, w in enumerate(layer.weights):\n",
    "    print(f\"Weight {i} gradient: {w.grad}\")\n",
    "if layer.has_bias:\n",
    "    for i, b in enumerate(layer.bias):\n",
    "        print(f\"Bias {i} gradient: {b.grad}\")\n",
    "for i, xi in enumerate(x):\n",
    "    print(f\"Input {i} gradient: {xi.grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f57bfb",
   "metadata": {},
   "source": [
    "Result!\n",
    "\n",
    "We can see that after 2nd pass we got exactly what we wanted. We changed the loss function only by `-0.5*out[0]`. \n",
    "\n",
    "This effectively changes the derivative w.r.t each weight and bias. For example, the derivative of `-0.5*out[0] `contributes -0.5 times the original derivative of `out[0]`, hence the halving of some gradients like weight 0, weight 2, weight 3, bias 0, and inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476fc278",
   "metadata": {},
   "source": [
    "### Sequential Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8f183",
   "metadata": {},
   "source": [
    "Sequential Container is the next abstruction step in architecture allowing us to stack multiple layers in a linear fashion. \n",
    "\n",
    "In Andrew’s original MLP, each layer (made of neurons) is manually chained in the `__call__` function:\n",
    "```python\n",
    "def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "        x = layer(x)\n",
    "    return x\n",
    "```\n",
    "\n",
    "This works fine, but as we add more complex layers (like Dropout or LayerNorm), we want a cleaner, modular way to stack them, similar to how PyTortch's `nn.Sequential` works.\n",
    "\n",
    "A `Sequential` container allows us to:\n",
    "\n",
    "* Chain multiple layers in order\n",
    "* Treat the entire sequence as a single module\n",
    "* Easily collect parameters from all internal layers for optimization\n",
    "\n",
    "For example we can have single Sequential Layer look like this:\n",
    "```python\n",
    "model = Sequential(\n",
    "    Linear(2, 8),\n",
    "    GELU(), # hypothetical activation function\n",
    "    Linear(8, 1)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81da9c7",
   "metadata": {},
   "source": [
    "**Reference:** https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e0123e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement Sequential container for micrograd pro:\n",
    "class Sequential(Layer):\n",
    "    def __init__(self, *layers):\n",
    "        # Store layers in a list\n",
    "        self.layers = layers\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # Pass input through each layer in sequence\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        # Collect parameters from all layers\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params\n",
    "    \n",
    "    # Also add repr for better visualization\n",
    "    def __repr__(self):\n",
    "        layer_reprs = \", \".join([layer.__class__.__name__ for layer in self.layers])\n",
    "        return f\"Sequential({layer_reprs})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d1a36767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: [Value(data=3.6028742119667463, grad=0)]\n",
      "Model number of parameters: 49\n"
     ]
    }
   ],
   "source": [
    "# Let's test with a simple example:\n",
    "# Create a simple model with 2 Linear layers\n",
    "model = Sequential(\n",
    "    Linear(4, 8),  # Input layer: 4 features to 8 neurons\n",
    "    Linear(8, 1)   # Output layer: 8 neurons to 1 output features\n",
    ")\n",
    "\n",
    "x = [Value(0.5), Value(-1.5), Value(2.0), Value(1.0)]  # Example input with 4 features\n",
    "output = model(x)  # Forward pass\n",
    "print(\"Model output:\", output)\n",
    "print(\"Model number of parameters:\", len(model.parameters())) # should be 4*8 + 8 + 8*1 + 1 = 32 + 8 + 8 + 1 = 49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059347e6",
   "metadata": {},
   "source": [
    "### LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae58a8",
   "metadata": {},
   "source": [
    "Many of you probably heard of Batch Normalization, I definetly heard and used it before understanding Layer Normalization.\n",
    "\n",
    "The original problem accured 10 or even more years ago, when neural networks became deeper and deeper, and researchers noticed that activations in intermediate layers often *drift*, some neurons produce very large outputs, while others saturate. That's when **Batch Normalization** was introduced in 2015. It literally normalizes activations across the *batch dimension*.\n",
    "\n",
    "However, BatchNorm has limitations:\n",
    "- It depends on batch size, small batches make it unreliable\n",
    "- It behaves differently during training and **inference**\n",
    "- It's not well-suited for **RRNs** or **Transformers**\n",
    "\n",
    "*Reference:* BatchNorm - https://arxiv.org/pdf/1502.03167\n",
    "\n",
    "Thus a Layer Normalization came to life. A Better Alternative \n",
    "\n",
    "Literally Layer Normalization was proposed as a batch-size-independent alternative.\n",
    "Instead of normalizing across samples, it normalizes across the features of a single sample, treating every individual input vector as its own \"mini-batch.\"\n",
    "\n",
    "So for a single input $x = [x_1, x_2, ... , x_d]$:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i, \\quad\n",
    "\\sigma^2 = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "Then, the normalized and scaled outputs are:\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}, \\quad\n",
    "y_i = \\gamma_i \\hat{x}_i + \\beta_i\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $\\hat{x}_i$ (gain) and  $\\beta_i$ (bias) are learnable parameters per feature\n",
    "- $\\epsilon$ ensures numerical stability\n",
    "\n",
    "**Note:** Unlike Batch Norm,\n",
    "- There is no dependency on batch statistics\n",
    "- It works identically during training and inference\n",
    "- It can be used in any setting (transformers, RNNs, etc.)\n",
    "\n",
    "*Reference:* LayerNorm - https://arxiv.org/pdf/1607.06450"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff10bafc",
   "metadata": {},
   "source": [
    "### Why LayerNorm?\n",
    "\n",
    "LayerNorm became a **cornerstone** in modern deep learning, especially:\n",
    "- In **Transformers** (e.g., BERT, GPT, ViT), applied before or after each attention/feed-forward block\n",
    "- In **language models** and **sequence models**, where per-token normalization ensures stable gradients\n",
    "- In small-batch or online learning setups, since BatchNorm fails in small batch sizes\n",
    "\n",
    "Typical structure in transformer layers looks like this:\n",
    "\n",
    "```python\n",
    "x = x + MLP(LayerNorm(x))\n",
    "\n",
    "# or sometimes\n",
    "\n",
    "x = LayerNorm(x + MLP(x))\n",
    "```\n",
    "\n",
    "There are several reasons why I wanted to implement LayerNorm in `micrograd_pro`\n",
    "\n",
    "1. It's **relevant**, used in today's modern technologies\n",
    "\n",
    "2. **Batch independence**, micrograd and micrograd_pro are scalar-based and process one sample at a time\n",
    "\n",
    "3. **Conceptual clarity**, LayerNorm focuses on the feature dimension, aligning perfectly with how our `Value` based computation graph represents individual samples.\n",
    "\n",
    "So, with all that in mind, let's finalize:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mu$ = mean across features  \n",
    "- $\\sigma^2$ = variance across features  \n",
    "- $\\gamma, \\beta$ = trainable parameters\n",
    "\n",
    "**Reference**: PyTorch LayerNorm - https://docs.pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "86e0f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's implement LayerNorm for micrograd pro:\n",
    "class LayerNorm(Layer):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        # normalized_shape can be an int or a tuple of ints\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = [Value(1.0) for _ in range(np.prod(normalized_shape))]  # Scale parameter\n",
    "        self.beta = [Value(0.0) for _ in range(np.prod(normalized_shape))]   # Shift parameter\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # a little bit of math here\n",
    "        # x is a list of Value objects\n",
    "        # 1. Calculate mean\n",
    "        mean = sum(xi.data for xi in x) / len(x)\n",
    "        # 2. Calculate variance\n",
    "        variance = sum((xi.data - mean) ** 2 for xi in x) / len(x)\n",
    "        # 3. Normalize\n",
    "        normalized = [(xi.data - mean) / np.sqrt(variance + self.eps) for xi in x]\n",
    "        # 4. Scale and shift\n",
    "        out = [Value(gam * norm + bet, (xi,), 'layernorm') for xi, norm, gam, bet in zip(x, normalized, self.gamma, self.beta)]\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return super().parameters() + self.gamma + self.beta\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb7913",
   "metadata": {},
   "source": [
    "### Derivation LayerNorm\n",
    "\n",
    "Same way as with Linear layer we need to take care of gradients.\n",
    "\n",
    "LayerNorm gradients are trickier because the output depends on **all inputs simultaneously** (mean and variance), so each input's gradient is affected by every other input. However, conceptually it's the same workflow as Linear: define the forward pass, then implement the backward pass using the chain rule\n",
    "\n",
    "There is more math involved in this solution, so I would like to go a little bit deeper into it than the one for Linear layer. Please don't be scared of math!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e70ba",
   "metadata": {},
   "source": [
    "So my intuition would be this. \n",
    "\n",
    "Given 2 formulas of normalization step and learnable affine transformation:\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}, \\quad\n",
    "y_i = \\gamma_i \\hat{x}_i + \\beta_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mu = \\frac{1}{d}\\sum_i{x_i}$\n",
    "\n",
    "- $\\sigma^2 = \\frac{1}{d}\\sum_i{(x_i - \\mu)^2}$\n",
    "\n",
    "- $d = \\text{number of features}$\n",
    "\n",
    "- $\\gamma_i, \\beta_i \\ are \\ learnable$ - scale and shift\n",
    "\n",
    "\n",
    "Thus, we need to find:\n",
    "\n",
    "1.\t$\\frac{\\partial y_i}{\\partial \\gamma_i} = \\hat{x}_i$ - gradient for gamma\n",
    "2.\t$\\frac{\\partial y_i}{\\partial \\beta_i} = 1$ - gradient for beta\n",
    "3.\t$\\frac{\\partial y_i}{\\partial x_i}$ - more complex because $x_i$ affects $\\hat{x}_i$ through both $\\mu$ and $\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d043c0dd",
   "metadata": {},
   "source": [
    "So to derive gradients for $x_i$ , $\\gamma_i$, and $\\beta_i$, given the loss $L$:\n",
    "\n",
    "1. $\\frac{\\partial L}{\\partial y_i} = g_i$\n",
    "\n",
    "2. $\\frac{\\partial L}{\\partial \\beta_i} = g_i$\n",
    "\n",
    "3. $\\frac{\\partial L}{\\partial \\gamma_i} = g_i \\cdot \\hat{x}_i$\n",
    "\n",
    "\n",
    "Now we want the gradient of the loss $L$ with respect to an input feature $x_i$ for Layer Normalization.\n",
    "\n",
    "Recall the forward equations for one input vector $\\mathbf{x}=(x_1,\\dots,x_d)$:\n",
    "\n",
    "\n",
    "$\\mu = \\frac{1}{d}\\sum_{j=1}^{d} x_j, \\qquad \\sigma^2 = \\frac{1}{d}\\sum_{j=1}^{d} (x_j - \\mu)^2,$ \n",
    "\n",
    "$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}, \\qquad y_i = \\gamma_i \\hat{x}_i + \\beta_i.$\n",
    "\n",
    "Let, $g_i \\equiv \\frac{\\partial L}{\\partial y_i}$ be the upstream gradient at $y_i$.\n",
    "\n",
    "From the affine transform $y_i = \\gamma_i \\hat{x}_i + \\beta_i$ \n",
    "\n",
    "We get:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\beta_i} = g_i, \\qquad \\frac{\\partial L}{\\partial \\gamma_i} = g_i \\hat{x}_i,$ \n",
    "\n",
    "and the gradient flowing into $(\\hat{x}_i)$ is $\\hat{g}_i \\equiv \\frac{\\partial L}{\\partial \\hat{x}_i} = g_i \\gamma_i.$\n",
    "\n",
    "So the remaining task is to compute $(\\dfrac{\\partial L}{\\partial x_i} = \\sum_{k=1}^{d} \\hat{g}_k \\dfrac{\\partial \\hat{x}_k}{\\partial x_i}).$\n",
    "\n",
    "---\n",
    "\n",
    "1. **We want to compute $(\\dfrac{\\partial \\hat{x}_k}{\\partial x_i})$**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{x}_k}{\\partial x_i}\n",
    "= \\frac{\\partial}{\\partial x_i}\\left(\\frac{x_k - \\mu}{s}\\right)\n",
    "= \\frac{1}{s}\\frac{\\partial (x_k - \\mu)}{\\partial x_i}\n",
    "(x_k - \\mu) \\frac{\\partial (1/s)}{\\partial x_i}.\n",
    "$$\n",
    "\n",
    "\n",
    "What I did here is write $(\\hat{x}_k = \\dfrac{x_k - \\mu}{s})$ with $(s \\equiv \\sqrt{\\sigma^2 + \\epsilon}).$\n",
    "\n",
    "\n",
    "Then handle this derivative piecewise. \n",
    "\n",
    "First, note easy part:\n",
    "\n",
    "We know $\\mu = \\frac{1}{d}\\sum_j x_j$, so\n",
    "\n",
    "$\\frac{\\partial \\mu}{\\partial x_i} = \\frac{1}{d}.$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (x_k - \\mu)}{\\partial x_i}\n",
    "= \\frac{\\partial x_k}{\\partial x_i} - \\frac{1}{d}\n",
    "= \\delta_{ki} - \\frac{1}{d}.\n",
    "$$\n",
    "\n",
    "\n",
    "After that, find $\\frac{\\partial s}{\\partial x_i}$\n",
    "\n",
    "We know for the variance, $\\sigma^2 = \\frac{1}{d}\\sum_j (x_j - \\mu)^2,$ differentiate w.r.t. $x_i$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial s}{\\partial x_i} = \\frac{1}{2s} \\cdot \\frac{2}{d}\\sum_j (x_j - \\mu) \\frac{\\partial (x_j - \\mu)}{\\partial x_i}.\n",
    "$$\n",
    "\n",
    "From before:\n",
    "\n",
    "$$\\frac{\\partial s}{\\partial x_i}\n",
    "= \\frac{1}{s d}\\sum_j (x_j - \\mu)(\\delta_{ji} - \\tfrac{1}{d}).$$\n",
    "\n",
    "Simplify that:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial s}{\\partial x_i}\n",
    "= \\frac{1}{s d}\\left((x_i - \\mu) - \\tfrac{1}{d}\\sum_j (x_j - \\mu)\\right)\n",
    "= \\frac{1}{s d}(x_i - \\mu),\n",
    "$$\n",
    "\n",
    "Because $(\\sum_j (x_j-\\mu)=0)$, the cross term vanishes and you get $\\frac{\\partial \\sigma^2}{\\partial x_i} = \\frac{2}{d}(x_i - \\mu).$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "  \\frac{\\partial s}{\\partial x_i} = \\frac{1}{2s}\\frac{\\partial \\sigma^2}{\\partial x_i}\n",
    "  = \\frac{1}{2s} \\cdot \\frac{2}{d}(x_i-\\mu)\n",
    "  = \\frac{x_i - \\mu}{d\\,s}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (1/s)}{\\partial x_i}\n",
    "= -\\frac{1}{s^2} \\frac{\\partial s}{\\partial x_i}\n",
    "= -\\frac{(x_i - \\mu)}{s^3 d}.\n",
    "$$\n",
    "\n",
    "Combine two parts, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{x}k}{\\partial x_i}\n",
    "= \\frac{1}{s}(\\delta{ki} - \\tfrac{1}{d})\n",
    "\\frac{(x_k - \\mu)}{s} \\cdot \\frac{(x_i - \\mu)}{s^2 d}.\n",
    "$$\n",
    "\n",
    "Recognize that $\\hat{x}_k = \\frac{x_k - \\mu}{s},$ so:\n",
    "\n",
    "\n",
    "$$\n",
    "\\boxed{\\displaystyle\n",
    "\\frac{\\partial \\hat{x}_k}{\\partial x_i} \\;=\\; \\frac{1}{s}\\Big(\\delta_{ki} - \\tfrac{1}{d} - \\tfrac{1}{d}\\,\\hat{x}_k\\hat{x}_i\\Big)\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "2. **Combine with $(\\hat{g}_k)$ to get $(\\dfrac{\\partial L}{\\partial x_i})$**\n",
    "\n",
    "Now sum over $k$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i}\n",
    "= \\sum_{k=1}^d \\hat{g}_k \\frac{\\partial \\hat{x}_k}{\\partial x_i}\n",
    "= \\sum_k \\hat{g}_k \\cdot \\frac{1}{s}\\Big(\\delta_{ki} - \\tfrac{1}{d} - \\tfrac{1}{d}\\,\\hat{x}_k\\hat{x}_i\\Big).\n",
    "$$\n",
    "\n",
    "Separate the terms, distribute the sum, we get rid of Kronecker delta:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i}\n",
    "= \\frac{1}{s}\\Big(\n",
    "\\hat{g}_i\n",
    "- \\frac{1}{d}\\sum_k \\hat{g}_k\n",
    "- \\frac{\\hat{x}_i}{d}\\sum_k \\hat{g}_k \\hat{x}_k\n",
    "\\Big).\n",
    "$$\n",
    "\n",
    "Finally substitute $(\\hat{g}_k = g_k \\gamma_k)$ and $(s=\\sqrt{\\sigma^2+\\epsilon})$ to obtain the commonly used compact form:\n",
    "\n",
    "$$\n",
    "\\boxed{\\displaystyle\n",
    "\\frac{\\partial L}{\\partial x_i}\n",
    "=\n",
    "\\frac{1}{\\sqrt{\\sigma^2+\\epsilon}}\\left(\n",
    "g_i \\gamma_i\n",
    "- \\frac{1}{d}\\sum_{j=1}^d g_j \\gamma_j\n",
    "- \\frac{\\hat{x}_i}{d}\\sum_{j=1}^d g_j \\gamma_j \\hat{x}_j\n",
    "\\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Short intuition**\n",
    "\n",
    "- The first term $\\frac{g_i \\gamma_i}{s}$ (or $\\frac{g_i \\gamma_i}{\\sqrt{\\sigma^2+\\epsilon}}$ ) is the **direct** sensitivity of the loss to $x_i$.\n",
    "- The second term subtracts the **mean effect** because the mean $\\mu$ depends on every $x_j$.\n",
    "- The third term corrects for the effect of $x_i$ on the variance through $\\hat{x}_i$, which couples all features together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "498a703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement LayerNorm class with backward method: \n",
    "\n",
    "class LayerNorm(Layer):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        # normalized_shape can be an int or a tuple of ints\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        \n",
    "        n= int(np.prod(normalized_shape))\n",
    "        self.gamma = [Value(1.0) for _ in range(n)]  # Scale parameter\n",
    "        self.beta = [Value(0.0) for _ in range(n)]   # Shift parameter\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        x: list of Value objects (length d)\n",
    "        returns: list of Value objects y_i = gamma_i * hat_x_i + beta_i\n",
    "        where hat_x_i = (x_i - mu) / sqrt(sigma^2 + eps)\n",
    "        \"\"\"\n",
    "        d = len(x)\n",
    "        # 1. Calculate mean and variance as scalars\n",
    "        mean = sum(xi.data for xi in x) / d\n",
    "        var = sum((xi.data - mean) ** 2 for xi in x) / d\n",
    "        s = np.sqrt(var + self.eps) # s == sqrt(sigma^2 + eps)\n",
    "        \n",
    "        # 2. Compute normalized values hat_x as floats\n",
    "        hat = [(xi.data - mean) / s for xi in x]\n",
    "        \n",
    "        # 3. Crate output Value objects (data computed from gamma and beta)\n",
    "        outs = []\n",
    "        for i, (xi, gi, bi) in enumerate(zip(x, self.gamma, self.beta)):\n",
    "            out_val = Value(gi.data * hat[i] + bi.data, _children=(xi, gi, bi), _op='layernorm')\n",
    "            outs.append(out_val)\n",
    "            \n",
    "        # 4. Define backward for each output. Each backward uses *current* outs[j].grad\n",
    "        # since xi.grad depends on sums over all g_j * gamma_j etc.\n",
    "        def make_backward(i, xi=None, gi=None, bi=None):\n",
    "            # bind referenced objects in closure\n",
    "            xi = x[i]\n",
    "            gi = self.gamma[i]\n",
    "            bi = self.beta[i]\n",
    "            def _backward():\n",
    "                # upstream gradients for y_j (these may have been accumulated already)\n",
    "                g_list = [oj.grad for oj in outs] # g_j = dL/dy_j\n",
    "                \n",
    "                # precompute sums used in formuala:\n",
    "                # sum1 = sum_j g_j * gamma_j\n",
    "                # sum2 = sum_j g_j * gamma_j * hat_x_j\n",
    "                sum1, sum2 = 0.0, 0.0\n",
    "                for j, gj in enumerate(g_list):\n",
    "                    sum1 += gj * self.gamma[j].data\n",
    "                    sum2 += gj * self.gamma[j].data * hat[j]\n",
    "                    \n",
    "                # local values\n",
    "                s_val = s # sqrt(var + eps)\n",
    "                hat_i = hat[i] # hat_x_i\n",
    "                g_i = g_list[i] # dL/dy_i\n",
    "                gamma_i = gi.data # gamma_i\n",
    "                \n",
    "                # d_beta_i = g_i\n",
    "                bi.grad += g_i\n",
    "                \n",
    "                # d_gamma_i = g_i * hat_i\n",
    "                gi.grad += g_i * hat_i\n",
    "                \n",
    "                # d_x_i = (1/s) * (g_i * gamma_i - (1/d) * sum1 - (hat_i/d) * sum2)\n",
    "                dx_i = (1 / s_val) * (g_i * gamma_i - (1/d) * sum1 - (hat_i / d) * sum2)\n",
    "                xi.grad += dx_i\n",
    "                \n",
    "            return _backward\n",
    "\n",
    "        for i, out_val in enumerate(outs):\n",
    "            out_val._backward = make_backward(i)\n",
    "            \n",
    "        return outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        return super().parameters() + self.gamma + self.beta        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5de7b623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm output: [Value(data=-2.529817068505626, grad=0), Value(data=0.05131859931039018, grad=0), Value(data=-0.3675457328735935, grad=0), Value(data=1.1324542671264064, grad=0)]\n",
      "\n",
      "Gradients after first backward pass:\n",
      "x grads: [np.float64(0.03162448422118484), np.float64(-0.06324454128020839), np.float64(0.06324454128020839), np.float64(-0.03162448422118484)]\n",
      "gamma grads: [np.float64(-1.264908534252813), np.float64(-0.6324542671264065), np.float64(0.6324542671264065), np.float64(1.264908534252813)]\n",
      "beta grads: [1, 1, 1, 1]\n",
      "\n",
      "LayerNorm output (2nd pass): [Value(data=-2.529817068505626, grad=0), Value(data=0.05131859931039018, grad=0), Value(data=-0.3675457328735935, grad=0), Value(data=1.1324542671264064, grad=0)]\n",
      "\n",
      "Gradients after second backward pass:\n",
      "x grads: [np.float64(0.08854577303167331), np.float64(-0.02529718406034597), np.float64(0.10435535884496894), np.float64(0.08221708147701244)]\n",
      "gamma grads: [np.float64(-1.3913993876780943), np.float64(-0.6324542671264065), np.float64(0.3794725602758439), np.float64(1.264908534252813)]\n",
      "beta grads: [1.1, 1, 0.6, 1]\n"
     ]
    }
   ],
   "source": [
    "# Let's test LayerNorm with a simple example:\n",
    "# Create inputs and LayerNorm\n",
    "x = [Value(1.0), Value(2.0), Value(4.0), Value(5.0)]\n",
    "ln = LayerNorm(4)\n",
    "\n",
    "# Set some \"learned\" gamma and beta to mimic training\n",
    "ln.gamma = [Value(2.0), Value(1.5), Value(1.0), Value(0.5)]\n",
    "ln.beta  = [Value(0.0), Value(1.0), Value(-1.0), Value(0.5)]\n",
    "\n",
    "# Forward pass\n",
    "out = ln(x)\n",
    "print(\"LayerNorm output:\", out)\n",
    "\n",
    "# Compute simple scalar loss (sum of outputs)\n",
    "loss = sum(out)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Inspect gradients\n",
    "print(\"\\nGradients after first backward pass:\")\n",
    "print(\"x grads:\", [xi.grad for xi in x])\n",
    "print(\"gamma grads:\", [g.grad for g in ln.gamma])\n",
    "print(\"beta grads:\", [b.grad for b in ln.beta])\n",
    "\n",
    "# Zero gradients to prepare for second pass\n",
    "ln.zero_grad()  # clears gamma and beta grads\n",
    "for xi in x:\n",
    "    xi.grad = 0\n",
    "\n",
    "# Forward pass again \n",
    "out = ln(x)\n",
    "print(\"\\nLayerNorm output (2nd pass):\", out)\n",
    "\n",
    "# Backward pass second pass\n",
    "# can also change loss slightly if you want to see different gradients\n",
    "loss = sum(out) + 0.1 * out[0] - 0.4 * out[2]\n",
    "loss.backward()\n",
    "\n",
    "# Inspect gradients after second pass\n",
    "print(\"\\nGradients after second backward pass:\")\n",
    "print(\"x grads:\", [xi.grad for xi in x])\n",
    "print(\"gamma grads:\", [g.grad for g in ln.gamma])\n",
    "print(\"beta grads:\", [b.grad for b in ln.beta])\n",
    "\n",
    "# inspect changes in gradients considering out[0] and out[2] contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a11f6612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm output: [Value(data=-1.3416354199689269, grad=0), Value(data=-0.447211806656309, grad=0), Value(data=0.447211806656309, grad=0), Value(data=1.3416354199689269, grad=0)]\n",
      "LayerNorm number of parameters: 8\n"
     ]
    }
   ],
   "source": [
    "# let's test LayerNorm with a simple example:\n",
    "layer_norm = LayerNorm(normalized_shape=4)  # Normalize over 4 features\n",
    "x = [Value(1.0), Value(2.0), Value(3.0), Value(4.0)]\n",
    "output = layer_norm(x)\n",
    "print(\"LayerNorm output:\", output)\n",
    "print(\"LayerNorm number of parameters:\", len(layer_norm.parameters()))  # should be 8 because we have 4 gamma and 4 beta\n",
    "\n",
    "# x = [Value(1.0), Value(2.0), Value(3.0), Value(4.0)]\n",
    "# Mean = (1 + 2 + 3 + 4) / 4 = 2.5\n",
    "# Variance = [(1-2.5)² + (2-2.5)² + (3-2.5)² + (4-2.5)²] / 4 = [2.25 + 0.25 + 0.25 + 2.25] / 4 = 5 / 4 = 1.25\n",
    "# Standard Deviation = sqrt(1.25 + 1e-5) ≈ 1.118\n",
    "# Normalized values:\n",
    "# (1 - 2.5) / 1.118 ≈ -1.3416\n",
    "# (2 - 2.5) / 1.118 ≈ -0.4472\n",
    "# (3 - 2.5) / 1.118 ≈ 0.4472\n",
    "# (4 - 2.5) / 1.118 ≈ 1.3416\n",
    "# Since gamma is initialized to 1 and beta to 0, the final output values will be approximately:\n",
    "# Value(-1.3416), Value(-0.4472), Value(0.4472), Value(1.3416)\n",
    "# After gamma and beta are \"learned\" during training, the output values will be scaled and shifted accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd9a1d",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdf1853",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique used to prevent overfitting.\n",
    "\n",
    "Basically during training, it randomly \"drops\" (sets to zero) a fraction of neurons` outputs in a layer, so that the network cannot rely too heavily on a single neuron.\n",
    "\n",
    "**Note:** \n",
    "- During inference, no neurons are dropped, but outputs are typically scaled to account for the dropped fraction during training\n",
    "\n",
    "How it works (training vs inference):\n",
    "- Training: Each neuron has probability p of being kept (or 1-p of being dropped). Multiply the remaining outputs by 1/p to keep expected value unchanged.\n",
    "- Inference: Keep all neurons active, no scaling needed (or just multiply by p depending on convention).\n",
    "\n",
    "**Reference:**\n",
    "- Original paper: https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "- torch.nn.Dropout: https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03792424",
   "metadata": {},
   "source": [
    "![dropout](../pics/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9ad475",
   "metadata": {},
   "source": [
    "### Why Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83635d7",
   "metadata": {},
   "source": [
    "- Forces the network to learn redundant representations of the data\n",
    "- Reduces co-adaptation of neurons, each neuron has to be useful independently\n",
    "- Works well in fully connected layers, and sometimes in other layers like embeddings\n",
    "\n",
    "\n",
    "\n",
    "Even though dropout is mostly historical / pedagogical in **modern** deep learning, I think it's still valuable to understand, experiment with it and do some testing in micrograd engine.\n",
    "\n",
    "- For NLP in BERT, GPT, etc. dropout is mostly applied only to **embeddings or attention weights**, and sometimes only in the feed-forward blocks\n",
    "- In Diffusion models rarely used since mechanisms like **noise injection** serve a similar regularization role\n",
    "- In general dropout nowadays is not as important since models are trained with **very large datasets**, so overfitting is less of an issue. \n",
    "\n",
    "**However**: I still think it's an important concept and we shouldn't skip it entirely! Especially since in context of micrograd we are dealing with small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7a6b720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement dropout for micrograd pro:\n",
    "# need to import random module\n",
    "import random\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, p=0.1, seed=None):\n",
    "        self.p = p # dropout probability\n",
    "        self.training = True # by default, we are in training mode\n",
    "        # wanted to implement seed as well\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            # dropout mask is for each element in x, 0 with prob p, 1 with prob (1-p)\n",
    "            self.mask = [0 if random.random() < self.p else 1 for _ in x]  # Store mask for backward pass\n",
    "            # from wikipedia bernoulli distribution\n",
    "            out = [xi * m / (1 - self.p) for xi, m in zip(x, self.mask)]\n",
    "        else:\n",
    "            out = x  # during evaluation, do nothing\n",
    "    \n",
    "        # we don't need to implement backward since dropout is not learnable and does not have parameters\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def parameters(self):\n",
    "        return super().parameters()  # no parameters in dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "867c4e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [1.3333333333333333, 0.0, 4.0, 0.0]\n",
      "Mask used: [1, 0, 1, 0]\n",
      "Input grads: [1.3333333333333333, 0.0, 1.3333333333333333, 0.0]\n",
      "Eval output: [1.0, 2.0, 3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "# let's test it:\n",
    "# assume Value and Layer are defined above\n",
    "x = [Value(1.0), Value(2.0), Value(3.0), Value(4.0)]\n",
    "\n",
    "drop = Dropout(p=0.25, seed=42)  # reproducible\n",
    "drop.training = True\n",
    "\n",
    "out = drop(x)\n",
    "print(\"Output:\", [o.data for o in out])\n",
    "print(\"Mask used:\", drop.mask)\n",
    "\n",
    "# simple loss (sum of outputs)\n",
    "loss = sum(out)\n",
    "loss.backward()\n",
    "\n",
    "# \tp = 0.25 (drop probability)\n",
    "# Keep probability = 1 - p = 0.75\n",
    "# Scaling factor = 1 / (1 - p) = 1 / 0.75 = 4/3 or 1.3333\n",
    "print(\"Input grads:\", [xi.grad for xi in x])\n",
    "\n",
    "# Turn off dropout for inference\n",
    "drop.training = False\n",
    "out_eval = drop(x)\n",
    "print(\"Eval output:\", [o.data for o in out_eval])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846b9a05",
   "metadata": {},
   "source": [
    "# <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Building Custom Optimizer</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c129f",
   "metadata": {},
   "source": [
    "To be added later..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ccba82",
   "metadata": {},
   "source": [
    "# <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Conclusion</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aafc8da",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- Enhanced the original micrograd engine with:\n",
    "\t- Custom activation functions: GELU, LeakyReLU, tanh\n",
    "\t- A flexible Linear layer with bias handling\n",
    "\t- Sequential container for stacking layers\n",
    "\t- LayerNorm with correct backward implementation\n",
    "\t- Dropout with training/eval modes and seeding\n",
    "- Built a foundation for experimenting with gradient flow and MLP architectures.\n",
    "- This notebook sets the stage for adding custom optimizers (SGD, Adam, RMSProp)\n",
    "\n",
    "\n",
    "### Things to do next\n",
    "- Check out my `gradient_flow` notebook with:\n",
    "\t- Training loops and loss tracking\n",
    "\t- Visualization of gradient magnitudes and flow\n",
    "\n",
    "\n",
    "### Acknowledgements / References\n",
    "- micrograd repository — core inspiration\n",
    "- Papers & concepts referenced for activations and normalization:\n",
    "\t- Hendrycks & Gimpel (2016), Gaussian Error Linear Units (GELU)\n",
    "\t- Ba et al. (2016), Layer Normalization\n",
    "\t- Maas, A. L., Hannun, A. Y., & Ng, A. Y. (2013). Rectifier Nonlinearities Improve Neural Network Acoustic Models\n",
    "\t- He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n",
    "\t- Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting.\n",
    "\t- possibly more, check out resources.md in the gitHub repo for this project\n",
    "Overfitting\n",
    "- PyTorch documentation\n",
    "\n",
    "**Note:** This notebook was developed as a learning and experimentation tool, not production-ready code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6954df13",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
