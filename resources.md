## Papers

- [GELU paper](https://arxiv.org/pdf/1606.08415v3)

- [BatchNorm paper](https://arxiv.org/pdf/1502.03167)

- [LayerNorm paper](https://arxiv.org/pdf/1607.06450)

- [Original Dropout paper](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)

- [SGD paper](https://arxiv.org/pdf/1609.04747)

## Others

### Tutorials / Articles
- [GELU explained (Medium)](https://medium.com/@shauryagoel/gelu-gaussian-error-linear-unit-4ec59fb2e47c)
- [Probability course: Normal distribution](https://www.probabilitycourse.com/chapter4/4_2_3_normal.php)

### Documentation
- [PyTorch Sequential](https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html)
- [PyTorch Linear](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html)
- [PyTorch LayerNorm](https://docs.pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
- [PyTorch Dropout](https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html)
- [PyTorch ReLU / LeakyReLU / Tanh](https://pytorch.org/docs/stable/nn.html)
- [NVIDIA Fully Connected](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html)

### References
- [Wikipedia: Neural Networks](https://en.wikipedia.org/wiki/Neural_network_(machine_learning))
- [Wikipedia: Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)
- [Wikipedia: Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)